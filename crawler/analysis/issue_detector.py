"""
ì´ìŠˆ íƒì§€ ëª¨ë“ˆ

Requirements: 3.1, 3.2, 3.3, 3.4, 3.5
- ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ë°˜ë³µë˜ëŠ” ì´ìŠˆ íƒì§€
- í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§
- ì´ìŠˆ ìš°ì„ ìˆœìœ„í™”
- Hot Issue íƒì§€
"""

import uuid
import re
from typing import List, Dict, Set, Optional, Tuple
from collections import defaultdict
from datetime import datetime, timedelta

from crawler.models.data_models import PostContent
from crawler.models.analysis_models import (
    KeywordCluster, 
    DetectedIssue, 
    IssueSeverity,
    SentimentResult,
    HotPost
)
from crawler.analysis.keyword_extractor import KeywordExtractor
from crawler.analysis.sentiment import SentimentAnalyzer


class IssueDetector:
    """ì´ìŠˆ íƒì§€ê¸°
    
    Requirements: 3.1, 3.2, 3.3, 3.4, 3.5
    - ê²Œì‹œê¸€ì—ì„œ ë°˜ë³µë˜ëŠ” ì´ìŠˆ íƒì§€
    - í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ìœ ì‚¬ ì´ìŠˆ ê·¸ë£¹í™”
    - ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
    - Hot Issue íƒì§€
    """
    
    # ë²„ê·¸/ì˜¤ë¥˜ ê´€ë ¨ í‚¤ì›Œë“œ
    BUG_KEYWORDS = {
        "ë²„ê·¸", "ì˜¤ë¥˜", "ì—ëŸ¬", "ë ‰", "íŠ•ê¹€", "ì ‘ì†ë¶ˆê°€",
        "í¬ë˜ì‹œ", "í”„ë¦¬ì§•", "ë©ˆì¶¤", "ì•ˆë¨", "ë¶ˆê°€", "ì•ˆë¼",
        "ì˜¤ì‘ë™", "ì‘ë™ì•ˆí•¨", "ì‹¤í–‰ì•ˆë¨", "ë¡œë”©", "ë¬´í•œë¡œë”©",
        "ëŠê¹€", "íŒ…ê¹€", "ë‹¤ìš´", "ì„œë²„ë‹¤ìš´", "ì ê²€", "ê¸´ê¸‰ì ê²€",
        "bug", "error", "crash", "freeze", "lag"
    }
    
    def __init__(
        self, 
        sentiment_analyzer: Optional[SentimentAnalyzer] = None,
        keyword_extractor: Optional[KeywordExtractor] = None
    ):
        """ì´ìŠˆ íƒì§€ê¸° ì´ˆê¸°í™”
        
        Args:
            sentiment_analyzer: ê°ì„± ë¶„ì„ê¸° (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
            keyword_extractor: í‚¤ì›Œë“œ ì¶”ì¶œê¸° (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
        """
        self.sentiment_analyzer = sentiment_analyzer or SentimentAnalyzer()
        self.keyword_extractor = keyword_extractor or KeywordExtractor()
    
    def extract_keywords(self, posts: List[PostContent], top_n: int = 50) -> List[str]:
        """ê²Œì‹œê¸€ì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Requirements: 3.1
        - ê²Œì‹œê¸€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë°˜ë³µì ìœ¼ë¡œ ì–¸ê¸‰ë˜ëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Args:
            posts: ê²Œì‹œê¸€ ëª©ë¡
            top_n: ë°˜í™˜í•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
            
        Returns:
            í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ë¹ˆë„ìˆœ)
        """
        return self.keyword_extractor.extract_from_posts(posts, top_n)
    
    def _calculate_keyword_similarity(self, kw1: str, kw2: str) -> float:
        """ë‘ í‚¤ì›Œë“œ ê°„ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë¬¸ìì—´ ìœ ì‚¬ë„)
        
        Args:
            kw1: ì²« ë²ˆì§¸ í‚¤ì›Œë“œ
            kw2: ë‘ ë²ˆì§¸ í‚¤ì›Œë“œ
            
        Returns:
            ìœ ì‚¬ë„ (0.0 ~ 1.0)
        """
        # ë™ì¼í•œ ê²½ìš°
        if kw1 == kw2:
            return 1.0
        
        # í¬í•¨ ê´€ê³„
        if kw1 in kw2 or kw2 in kw1:
            return 0.8
        
        # ê³µí†µ ë¬¸ì ë¹„ìœ¨ (Jaccard ìœ ì‚¬ë„)
        set1 = set(kw1)
        set2 = set(kw2)
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        
        if union == 0:
            return 0.0
        
        return intersection / union
    
    def cluster_keywords(
        self, 
        keywords: List[str], 
        similarity_threshold: float = 0.5
    ) -> List[KeywordCluster]:
        """ìœ ì‚¬ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§
        
        Requirements: 3.2
        - ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ í‚¤ì›Œë“œë“¤ì„ ê·¸ë£¹í™”
        - ëª¨ë“  ì…ë ¥ í‚¤ì›Œë“œëŠ” ì •í™•íˆ í•˜ë‚˜ì˜ í´ëŸ¬ìŠ¤í„°ì— ì†í•¨
        
        Args:
            keywords: í‚¤ì›Œë“œ ëª©ë¡
            similarity_threshold: í´ëŸ¬ìŠ¤í„°ë§ ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            KeywordCluster ëª©ë¡
        """
        if not keywords:
            return []
        
        # ê° í‚¤ì›Œë“œê°€ ì–´ëŠ í´ëŸ¬ìŠ¤í„°ì— ì†í•˜ëŠ”ì§€ ì¶”ì 
        keyword_to_cluster: Dict[str, int] = {}
        clusters: List[List[str]] = []
        
        for keyword in keywords:
            # ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ì™€ ìœ ì‚¬ë„ í™•ì¸
            best_cluster_idx = -1
            best_similarity = 0.0
            
            for cluster_idx, cluster_keywords in enumerate(clusters):
                # í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ(ì²« ë²ˆì§¸)ì™€ ìœ ì‚¬ë„ ê³„ì‚°
                representative = cluster_keywords[0]
                similarity = self._calculate_keyword_similarity(keyword, representative)
                
                if similarity >= similarity_threshold and similarity > best_similarity:
                    best_similarity = similarity
                    best_cluster_idx = cluster_idx
            
            if best_cluster_idx >= 0:
                # ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€
                clusters[best_cluster_idx].append(keyword)
                keyword_to_cluster[keyword] = best_cluster_idx
            else:
                # ìƒˆ í´ëŸ¬ìŠ¤í„° ìƒì„±
                new_cluster_idx = len(clusters)
                clusters.append([keyword])
                keyword_to_cluster[keyword] = new_cluster_idx
        
        # KeywordCluster ê°ì²´ ìƒì„±
        result = []
        for idx, cluster_keywords in enumerate(clusters):
            cluster = KeywordCluster(
                cluster_id=f"cluster_{idx}",
                keywords=cluster_keywords,
                representative=cluster_keywords[0],  # ì²« ë²ˆì§¸ í‚¤ì›Œë“œê°€ ëŒ€í‘œ
                post_count=0,
                total_views=0,
                total_comments=0
            )
            result.append(cluster)
        
        return result
    
    def _update_cluster_stats(
        self, 
        cluster: KeywordCluster, 
        posts: List[PostContent]
    ) -> KeywordCluster:
        """í´ëŸ¬ìŠ¤í„° í†µê³„ ì—…ë°ì´íŠ¸
        
        Args:
            cluster: í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°
            posts: ê²Œì‹œê¸€ ëª©ë¡
            
        Returns:
            í†µê³„ê°€ ì—…ë°ì´íŠ¸ëœ í´ëŸ¬ìŠ¤í„°
        """
        post_count = 0
        total_views = 0
        total_comments = 0
        
        cluster_keywords_set = set(cluster.keywords)
        
        for post in posts:
            # ê²Œì‹œê¸€ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            post_keywords = set(self.keyword_extractor.extract_from_post(post, top_n=100))
            
            # í´ëŸ¬ìŠ¤í„° í‚¤ì›Œë“œì™€ êµì§‘í•©ì´ ìˆìœ¼ë©´ ê´€ë ¨ ê²Œì‹œê¸€
            if cluster_keywords_set & post_keywords:
                post_count += 1
                total_views += post.view_count
                total_comments += len(post.comments)
        
        return KeywordCluster(
            cluster_id=cluster.cluster_id,
            keywords=cluster.keywords,
            representative=cluster.representative,
            post_count=post_count,
            total_views=total_views,
            total_comments=total_comments
        )
    
    def calculate_priority(
        self, 
        cluster: KeywordCluster, 
        posts: List[PostContent],
        view_weight: float = 0.3,
        comment_weight: float = 0.3,
        frequency_weight: float = 0.4
    ) -> float:
        """ì´ìŠˆ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
        
        Requirements: 3.3
        - ì¡°íšŒìˆ˜, ëŒ“ê¸€ìˆ˜, ì–¸ê¸‰ ë¹ˆë„ë¥¼ ê°€ì¤‘ í•©ì‚°
        
        Args:
            cluster: í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°
            posts: ê²Œì‹œê¸€ ëª©ë¡
            view_weight: ì¡°íšŒìˆ˜ ê°€ì¤‘ì¹˜
            comment_weight: ëŒ“ê¸€ìˆ˜ ê°€ì¤‘ì¹˜
            frequency_weight: ì–¸ê¸‰ ë¹ˆë„ ê°€ì¤‘ì¹˜
            
        Returns:
            ìš°ì„ ìˆœìœ„ ì ìˆ˜ (0.0 ì´ìƒ)
        """
        if not posts:
            return 0.0
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_posts = len(posts)
        total_views = sum(p.view_count for p in posts)
        total_comments = sum(len(p.comments) for p in posts)
        
        # ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ê°’ (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        max_views = max(total_views, 1)
        max_comments = max(total_comments, 1)
        max_posts = max(total_posts, 1)
        
        # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
        view_score = cluster.total_views / max_views
        comment_score = cluster.total_comments / max_comments
        frequency_score = cluster.post_count / max_posts
        
        # ê°€ì¤‘ í•©ì‚°
        priority = (
            view_weight * view_score +
            comment_weight * comment_score +
            frequency_weight * frequency_score
        )
        
        return priority
    
    def _is_bug_related(self, post: PostContent) -> bool:
        """ë²„ê·¸/ì˜¤ë¥˜ ê´€ë ¨ ê²Œì‹œê¸€ ì—¬ë¶€ í™•ì¸
        
        Args:
            post: ê²Œì‹œê¸€ ê°ì²´
            
        Returns:
            ë²„ê·¸ ê´€ë ¨ ì—¬ë¶€
        """
        text = f"{post.title} {post.body}".lower()
        
        for keyword in self.BUG_KEYWORDS:
            if keyword.lower() in text:
                return True
        
        return False
    
    def classify_bug(self, post: PostContent) -> bool:
        """ë²„ê·¸/ì˜¤ë¥˜ ê´€ë ¨ ê²Œì‹œê¸€ ë¶„ë¥˜
        
        Requirements: 7.1, 7.2
        - "ë²„ê·¸", "ì˜¤ë¥˜", "ì—ëŸ¬", "ë ‰", "íŠ•ê¹€", "ì ‘ì†ë¶ˆê°€" ë“±ì˜ í‚¤ì›Œë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
        
        Args:
            post: ê²Œì‹œê¸€ ê°ì²´
            
        Returns:
            ë²„ê·¸ ê´€ë ¨ ì—¬ë¶€
        """
        return self._is_bug_related(post)
    
    def classify_bug_from_text(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì—ì„œ ë²„ê·¸/ì˜¤ë¥˜ ê´€ë ¨ ì—¬ë¶€ ë¶„ë¥˜
        
        Requirements: 7.1, 7.2
        - "ë²„ê·¸", "ì˜¤ë¥˜", "ì—ëŸ¬", "ë ‰", "íŠ•ê¹€", "ì ‘ì†ë¶ˆê°€" ë“±ì˜ í‚¤ì›Œë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë²„ê·¸ ê´€ë ¨ ì—¬ë¶€
        """
        if not text:
            return False
        
        text_lower = text.lower()
        
        for keyword in self.BUG_KEYWORDS:
            if keyword.lower() in text_lower:
                return True
        
        return False
    
    def get_bug_keywords_found(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ëœ ë²„ê·¸ í‚¤ì›Œë“œ ëª©ë¡ ë°˜í™˜
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            ë°œê²¬ëœ ë²„ê·¸ í‚¤ì›Œë“œ ëª©ë¡
        """
        if not text:
            return []
        
        text_lower = text.lower()
        found_keywords = []
        
        for keyword in self.BUG_KEYWORDS:
            if keyword.lower() in text_lower:
                found_keywords.append(keyword)
        
        return found_keywords
    
    def calculate_severity(
        self, 
        issue: DetectedIssue,
        max_post_count: int = 100,
        frequency_weight: float = 0.5,
        sentiment_weight: float = 0.5
    ) -> IssueSeverity:
        """ë²„ê·¸ ì‹¬ê°ë„ ê³„ì‚°
        
        Requirements: 7.4
        - ì–¸ê¸‰ ë¹ˆë„ì™€ ë¶€ì •ì  ê°ì„± ê°•ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¬ê°ë„ ê³„ì‚°
        - ì–¸ê¸‰ ë¹ˆë„ê°€ ë†’ê³  ë¶€ì •ì  ê°ì„±ì´ ê°•í• ìˆ˜ë¡ ì‹¬ê°ë„ê°€ ë†’ìŒ
        
        Args:
            issue: íƒì§€ëœ ì´ìŠˆ
            max_post_count: ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜ (ê¸°ë³¸ê°’: 100)
            frequency_weight: ì–¸ê¸‰ ë¹ˆë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.5)
            sentiment_weight: ê°ì„± ê°•ë„ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.5)
            
        Returns:
            ì‹¬ê°ë„ ë ˆë²¨
        """
        # ì–¸ê¸‰ ë¹ˆë„ ì •ê·œí™” (0 ~ 1)
        frequency_factor = min(1.0, issue.cluster.post_count / max(max_post_count, 1))
        
        # ë¶€ì •ì  ê°ì„± ê°•ë„ (0 ~ 1)
        # sentiment_avgëŠ” -1.0 ~ 1.0 ë²”ìœ„, ë¶€ì •ì ì¼ìˆ˜ë¡ ë†’ì€ ê°’ìœ¼ë¡œ ë³€í™˜
        sentiment_factor = max(0.0, -issue.sentiment_avg)  # 0 ~ 1
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        severity_score = (
            frequency_weight * frequency_factor +
            sentiment_weight * sentiment_factor
        )
        
        # ì‹¬ê°ë„ ê²°ì •
        if severity_score >= 0.7:
            return IssueSeverity.CRITICAL
        elif severity_score >= 0.5:
            return IssueSeverity.HIGH
        elif severity_score >= 0.3:
            return IssueSeverity.MEDIUM
        else:
            return IssueSeverity.LOW
    
    def calculate_severity_from_metrics(
        self,
        post_count: int,
        sentiment_avg: float,
        max_post_count: int = 100,
        frequency_weight: float = 0.5,
        sentiment_weight: float = 0.5
    ) -> IssueSeverity:
        """ë©”íŠ¸ë¦­ ê°’ìœ¼ë¡œë¶€í„° ì§ì ‘ ì‹¬ê°ë„ ê³„ì‚°
        
        Requirements: 7.4
        - ì–¸ê¸‰ ë¹ˆë„ì™€ ë¶€ì •ì  ê°ì„± ê°•ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¬ê°ë„ ê³„ì‚°
        
        Args:
            post_count: ì–¸ê¸‰ ë¹ˆë„ (ê²Œì‹œê¸€ ìˆ˜)
            sentiment_avg: í‰ê·  ê°ì„± ì ìˆ˜ (-1.0 ~ 1.0)
            max_post_count: ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜
            frequency_weight: ì–¸ê¸‰ ë¹ˆë„ ê°€ì¤‘ì¹˜
            sentiment_weight: ê°ì„± ê°•ë„ ê°€ì¤‘ì¹˜
            
        Returns:
            ì‹¬ê°ë„ ë ˆë²¨
        """
        # ì–¸ê¸‰ ë¹ˆë„ ì •ê·œí™” (0 ~ 1)
        frequency_factor = min(1.0, post_count / max(max_post_count, 1))
        
        # ë¶€ì •ì  ê°ì„± ê°•ë„ (0 ~ 1)
        sentiment_factor = max(0.0, -sentiment_avg)
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        severity_score = (
            frequency_weight * frequency_factor +
            sentiment_weight * sentiment_factor
        )
        
        # ì‹¬ê°ë„ ê²°ì •
        if severity_score >= 0.7:
            return IssueSeverity.CRITICAL
        elif severity_score >= 0.5:
            return IssueSeverity.HIGH
        elif severity_score >= 0.3:
            return IssueSeverity.MEDIUM
        else:
            return IssueSeverity.LOW
    
    def detect_issues(
        self, 
        posts: List[PostContent],
        top_n_keywords: int = 50,
        similarity_threshold: float = 0.5
    ) -> List[DetectedIssue]:
        """ì´ìŠˆ íƒì§€ ë° ìš°ì„ ìˆœìœ„í™”
        
        Requirements: 3.1, 3.2, 3.3, 3.5
        - ê²Œì‹œê¸€ì—ì„œ ì´ìŠˆ íƒì§€
        - ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        
        Args:
            posts: ê²Œì‹œê¸€ ëª©ë¡
            top_n_keywords: ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
            similarity_threshold: í´ëŸ¬ìŠ¤í„°ë§ ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            DetectedIssue ëª©ë¡ (ìš°ì„ ìˆœìœ„ ë‚´ë¦¼ì°¨ìˆœ)
        """
        if not posts:
            return []
        
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self.extract_keywords(posts, top_n_keywords)
        
        if not keywords:
            return []
        
        # 2. í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§
        clusters = self.cluster_keywords(keywords, similarity_threshold)
        
        # 3. í´ëŸ¬ìŠ¤í„° í†µê³„ ì—…ë°ì´íŠ¸ ë° ì´ìŠˆ ìƒì„±
        issues = []
        keyword_post_mapping = self.keyword_extractor.get_keyword_post_mapping(posts, top_n_keywords)
        
        for cluster in clusters:
            # í´ëŸ¬ìŠ¤í„° í†µê³„ ì—…ë°ì´íŠ¸
            updated_cluster = self._update_cluster_stats(cluster, posts)
            
            # ê´€ë ¨ ê²Œì‹œê¸€ URL ìˆ˜ì§‘
            related_posts = set()
            for keyword in updated_cluster.keywords:
                if keyword in keyword_post_mapping:
                    related_posts.update(keyword_post_mapping[keyword])
            
            # ê´€ë ¨ ê²Œì‹œê¸€ì˜ ê°ì„± í‰ê·  ê³„ì‚°
            related_post_objects = [p for p in posts if p.url in related_posts]
            sentiment_avg = 0.0
            if related_post_objects:
                sentiment_avg = self.sentiment_analyzer.get_average_sentiment(related_post_objects)
            
            # ë²„ê·¸ ê´€ë ¨ ì—¬ë¶€ í™•ì¸
            is_bug = any(
                kw.lower() in self.BUG_KEYWORDS or 
                any(bug_kw in kw.lower() for bug_kw in self.BUG_KEYWORDS)
                for kw in updated_cluster.keywords
            )
            
            # ìµœì´ˆ ë°œê²¬ ì‹œê°„ (ê°€ì¥ ì˜¤ë˜ëœ ê´€ë ¨ ê²Œì‹œê¸€)
            first_seen = None
            if related_post_objects:
                posts_with_date = [p for p in related_post_objects if p.created_at]
                if posts_with_date:
                    first_seen = min(p.created_at for p in posts_with_date)
            
            # ìš°ì„ ìˆœìœ„ ê³„ì‚°
            priority_score = self.calculate_priority(updated_cluster, posts)
            
            # ì´ìŠˆ ìƒì„±
            issue = DetectedIssue(
                issue_id=f"issue_{uuid.uuid4().hex[:8]}",
                title=updated_cluster.representative,
                cluster=updated_cluster,
                priority_score=priority_score,
                is_hot=False,  # ë‚˜ì¤‘ì— detect_hot_issuesì—ì„œ ì„¤ì •
                is_bug=is_bug,
                severity=IssueSeverity.LOW,  # ë‚˜ì¤‘ì— ì„¤ì •
                related_posts=list(related_posts),
                first_seen=first_seen,
                sentiment_avg=sentiment_avg
            )
            
            # ë²„ê·¸ì¸ ê²½ìš° ì‹¬ê°ë„ ê³„ì‚°
            if is_bug:
                issue.severity = self.calculate_severity(issue, max_post_count=len(posts))
            
            issues.append(issue)
        
        # 4. ìš°ì„ ìˆœìœ„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        issues.sort(key=lambda x: x.priority_score, reverse=True)
        
        return issues
    
    def detect_hot_issues(
        self, 
        issues: List[DetectedIssue], 
        threshold_percentile: float = 0.9
    ) -> List[DetectedIssue]:
        """Hot Issue íƒì§€
        
        Requirements: 3.4
        - Issue_Priority ìƒìœ„ 10%ë¥¼ Hot_Issueë¡œ ë¶„ë¥˜
        
        Args:
            issues: ì´ìŠˆ ëª©ë¡
            threshold_percentile: Hot Issue ì„ê³„ ë°±ë¶„ìœ„ (ê¸°ë³¸ê°’: 0.9 = ìƒìœ„ 10%)
            
        Returns:
            Hot Issue ëª©ë¡ (is_hot=Trueë¡œ ì„¤ì •ë¨)
        """
        if not issues:
            return []
        
        # ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬
        sorted_issues = sorted(issues, key=lambda x: x.priority_score, reverse=True)
        
        # ìƒìœ„ N% ê³„ì‚°
        hot_count = max(1, int(len(sorted_issues) * (1 - threshold_percentile)))
        
        # Hot Issue ì„¤ì •
        hot_issues = []
        for i, issue in enumerate(sorted_issues):
            if i < hot_count:
                issue.is_hot = True
                hot_issues.append(issue)
        
        return hot_issues
    
    def get_bug_issues(self, issues: List[DetectedIssue]) -> List[DetectedIssue]:
        """ë²„ê·¸ ê´€ë ¨ ì´ìŠˆ í•„í„°ë§
        
        Requirements: 7.3
        - ë²„ê·¸ ê´€ë ¨ ì´ìŠˆë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜
        
        Args:
            issues: ì´ìŠˆ ëª©ë¡
            
        Returns:
            ë²„ê·¸ ê´€ë ¨ ì´ìŠˆ ëª©ë¡ (ì‹¬ê°ë„ ìˆœ ì •ë ¬)
        """
        bug_issues = [issue for issue in issues if issue.is_bug]
        
        # ì‹¬ê°ë„ ìˆœìœ¼ë¡œ ì •ë ¬ (CRITICAL > HIGH > MEDIUM > LOW)
        severity_order = {
            IssueSeverity.CRITICAL: 0,
            IssueSeverity.HIGH: 1,
            IssueSeverity.MEDIUM: 2,
            IssueSeverity.LOW: 3
        }
        
        bug_issues.sort(key=lambda x: (severity_order.get(x.severity, 4), -x.priority_score))
        
        return bug_issues
    
    def detect_bug_issues(
        self, 
        posts: List[PostContent],
        top_n_keywords: int = 50,
        similarity_threshold: float = 0.5
    ) -> List[DetectedIssue]:
        """ë²„ê·¸ ê´€ë ¨ ì´ìŠˆë§Œ íƒì§€
        
        Requirements: 7.1, 7.2, 7.3, 7.4
        - ë²„ê·¸/ì˜¤ë¥˜ ê´€ë ¨ ê²Œì‹œê¸€ì—ì„œ ì´ìŠˆ íƒì§€
        - ì‹¬ê°ë„ ê³„ì‚° ë° ì •ë ¬
        
        Args:
            posts: ê²Œì‹œê¸€ ëª©ë¡
            top_n_keywords: ì¶”ì¶œí•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜
            similarity_threshold: í´ëŸ¬ìŠ¤í„°ë§ ìœ ì‚¬ë„ ì„ê³„ê°’
            
        Returns:
            ë²„ê·¸ ê´€ë ¨ DetectedIssue ëª©ë¡ (ì‹¬ê°ë„ ìˆœ)
        """
        # ì „ì²´ ì´ìŠˆ íƒì§€
        all_issues = self.detect_issues(posts, top_n_keywords, similarity_threshold)
        
        # ë²„ê·¸ ì´ìŠˆë§Œ í•„í„°ë§
        return self.get_bug_issues(all_issues)
    
    def get_bug_posts(self, posts: List[PostContent]) -> List[PostContent]:
        """ë²„ê·¸ ê´€ë ¨ ê²Œì‹œê¸€ë§Œ í•„í„°ë§
        
        Requirements: 7.1, 7.2
        - ë²„ê·¸/ì˜¤ë¥˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²Œì‹œê¸€ë§Œ ë°˜í™˜
        
        Args:
            posts: ê²Œì‹œê¸€ ëª©ë¡
            
        Returns:
            ë²„ê·¸ ê´€ë ¨ ê²Œì‹œê¸€ ëª©ë¡
        """
        return [post for post in posts if self.classify_bug(post)]
    
    def detect_urgent_issues(
        self, 
        posts: List[PostContent],
        hours: int = 24,
        min_posts: int = 10
    ) -> List[DetectedIssue]:
        """ê¸´ê¸‰ ì´ìŠˆ íƒì§€
        
        Requirements: 8.4
        - 24ì‹œê°„ ë‚´ ë™ì¼ ì´ìŠˆì— ëŒ€í•œ ê²Œì‹œê¸€ì´ 10ê°œ ì´ìƒì´ë©´ ê¸´ê¸‰ ì´ìŠˆ
        
        Args:
            posts: ê²Œì‹œê¸€ ëª©ë¡
            hours: ì‹œê°„ ë²”ìœ„ (ê¸°ë³¸ê°’: 24ì‹œê°„)
            min_posts: ìµœì†Œ ê²Œì‹œê¸€ ìˆ˜ (ê¸°ë³¸ê°’: 10ê°œ)
            
        Returns:
            ê¸´ê¸‰ ì´ìŠˆ ëª©ë¡
        """
        if not posts:
            return []
        
        # ìµœê·¼ Nì‹œê°„ ë‚´ ê²Œì‹œê¸€ í•„í„°ë§
        now = datetime.now()
        cutoff_time = now - timedelta(hours=hours)
        
        recent_posts = [
            p for p in posts 
            if p.created_at and p.created_at >= cutoff_time
        ]
        
        if len(recent_posts) < min_posts:
            return []
        
        # ì´ìŠˆ íƒì§€
        issues = self.detect_issues(recent_posts)
        
        # ê²Œì‹œê¸€ ìˆ˜ê°€ min_posts ì´ìƒì¸ ì´ìŠˆë§Œ ê¸´ê¸‰ ì´ìŠˆë¡œ ë¶„ë¥˜
        urgent_issues = [
            issue for issue in issues 
            if issue.cluster.post_count >= min_posts
        ]
        
        return urgent_issues

    def calculate_post_hot_score(
        self,
        post: PostContent,
        max_views: int,
        max_comments: int,
        view_weight: float = 0.4,
        comment_weight: float = 0.3,
        sentiment_weight: float = 0.3
    ) -> float:
        """ê²Œì‹œê¸€ì˜ Hot Score ê³„ì‚°
        
        Args:
            post: ê²Œì‹œê¸€ ê°ì²´
            max_views: ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ ì¡°íšŒìˆ˜
            max_comments: ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ ëŒ“ê¸€ìˆ˜
            view_weight: ì¡°íšŒìˆ˜ ê°€ì¤‘ì¹˜
            comment_weight: ëŒ“ê¸€ìˆ˜ ê°€ì¤‘ì¹˜
            sentiment_weight: ê°ì„± ê°€ì¤‘ì¹˜ (ë¶€ì •ì ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            
        Returns:
            Hot Score (0.0 ~ 1.0)
        """
        # ì¡°íšŒìˆ˜ ì •ê·œí™”
        view_score = post.view_count / max(max_views, 1)
        
        # ëŒ“ê¸€ìˆ˜ ì •ê·œí™”
        comment_count = len(post.comments)
        comment_score = comment_count / max(max_comments, 1)
        
        # ê°ì„± ì ìˆ˜ (ë¶€ì •ì ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜ - ë¬¸ì œ ê²Œì‹œê¸€ ìš°ì„ )
        sentiment_result = self.sentiment_analyzer.analyze_post(post)
        # -1.0 ~ 1.0ì„ 0.0 ~ 1.0ìœ¼ë¡œ ë³€í™˜ (ë¶€ì •ì ì¼ìˆ˜ë¡ ë†’ìŒ)
        sentiment_score = (1.0 - sentiment_result.score) / 2.0
        
        # ê°€ì¤‘ í•©ì‚°
        hot_score = (
            view_weight * min(view_score, 1.0) +
            comment_weight * min(comment_score, 1.0) +
            sentiment_weight * sentiment_score
        )
        
        return min(hot_score, 1.0)

    def detect_hot_posts(
        self,
        posts: List[PostContent],
        top_n: int = 20,
        threshold_percentile: float = 0.9,
        view_weight: float = 0.4,
        comment_weight: float = 0.3,
        sentiment_weight: float = 0.3
    ) -> List[HotPost]:
        """ê²Œì‹œê¸€ ê¸°ë°˜ Hot Post íƒì§€
        
        ê° ê²Œì‹œê¸€ì— ì ìˆ˜ë¥¼ ë¶€ì—¬í•˜ê³  ìƒìœ„ Nê°œ ë˜ëŠ” ìƒìœ„ 10%ë¥¼ Hot Postë¡œ ë°˜í™˜
        
        Args:
            posts: ê²Œì‹œê¸€ ëª©ë¡
            top_n: ë°˜í™˜í•  ìµœëŒ€ Hot Post ìˆ˜
            threshold_percentile: Hot Post ì„ê³„ ë°±ë¶„ìœ„ (ê¸°ë³¸ê°’: 0.9 = ìƒìœ„ 10%)
            view_weight: ì¡°íšŒìˆ˜ ê°€ì¤‘ì¹˜
            comment_weight: ëŒ“ê¸€ìˆ˜ ê°€ì¤‘ì¹˜
            sentiment_weight: ê°ì„± ê°€ì¤‘ì¹˜
            
        Returns:
            HotPost ëª©ë¡ (hot_score ë‚´ë¦¼ì°¨ìˆœ)
        """
        if not posts:
            return []
        
        # ì •ê·œí™”ë¥¼ ìœ„í•œ ìµœëŒ€ê°’ ê³„ì‚°
        max_views = max((p.view_count for p in posts), default=1)
        max_comments = max((len(p.comments) for p in posts), default=1)
        
        # ê° ê²Œì‹œê¸€ì˜ Hot Score ê³„ì‚°
        scored_posts = []
        for post in posts:
            hot_score = self.calculate_post_hot_score(
                post, max_views, max_comments,
                view_weight, comment_weight, sentiment_weight
            )
            
            # ê°ì„± ë¶„ì„
            sentiment_result = self.sentiment_analyzer.analyze_post(post)
            
            # ë²„ê·¸ ê´€ë ¨ ì—¬ë¶€
            is_bug = self._is_bug_related(post)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.keyword_extractor.extract_from_post(post, top_n=5)
            
            # ì‹¬ê°ë„ ê³„ì‚° (ë²„ê·¸ì¸ ê²½ìš°)
            severity = IssueSeverity.LOW
            if is_bug:
                # ë¶€ì •ì  ê°ì„± + ë†’ì€ hot_score = ë†’ì€ ì‹¬ê°ë„
                severity_score = (hot_score * 0.6) + (max(0, -sentiment_result.score) * 0.4)
                if severity_score >= 0.7:
                    severity = IssueSeverity.CRITICAL
                elif severity_score >= 0.5:
                    severity = IssueSeverity.HIGH
                elif severity_score >= 0.3:
                    severity = IssueSeverity.MEDIUM
            
            hot_post = HotPost(
                post_url=post.url,
                title=post.title,
                author=post.author or "",
                site=post.site,
                created_at=post.created_at,
                view_count=post.view_count,
                comment_count=len(post.comments),
                like_count=post.like_count,
                hot_score=hot_score,
                sentiment_score=sentiment_result.score,
                is_bug=is_bug,
                severity=severity,
                keywords=keywords
            )
            scored_posts.append(hot_post)
        
        # Hot Score ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        scored_posts.sort(key=lambda x: x.hot_score, reverse=True)
        
        # ìƒìœ„ N% ë˜ëŠ” top_n ì¤‘ ì‘ì€ ê°’
        hot_count = max(1, int(len(scored_posts) * (1 - threshold_percentile)))
        hot_count = min(hot_count, top_n, len(scored_posts))
        
        return scored_posts[:hot_count]

    def get_hot_posts_summary(
        self,
        posts: List[PostContent],
        top_n: int = 10
    ) -> Dict:
        """Hot Post ìš”ì•½ ì •ë³´ ë°˜í™˜
        
        Args:
            posts: ê²Œì‹œê¸€ ëª©ë¡
            top_n: ë°˜í™˜í•  Hot Post ìˆ˜
            
        Returns:
            ìš”ì•½ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        hot_posts = self.detect_hot_posts(posts, top_n=top_n)
        
        if not hot_posts:
            return {
                "total_hot_posts": 0,
                "hot_posts": [],
                "bug_count": 0,
                "critical_count": 0,
                "alert_message": None
            }
        
        bug_count = sum(1 for p in hot_posts if p.is_bug)
        critical_count = sum(1 for p in hot_posts if p.severity == IssueSeverity.CRITICAL)
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±
        alert_message = None
        top_post = hot_posts[0]
        if top_post.hot_score >= 0.7:
            alert_message = f"ğŸ”¥ ê¸´ê¸‰: '{top_post.title[:50]}...' - ì¡°íšŒìˆ˜ {top_post.view_count:,}, ëŒ“ê¸€ {top_post.comment_count}ê°œ"
        elif top_post.hot_score >= 0.5:
            alert_message = f"âš ï¸ ì£¼ëª©: '{top_post.title[:50]}...' - ì¡°íšŒìˆ˜ {top_post.view_count:,}"
        
        return {
            "total_hot_posts": len(hot_posts),
            "hot_posts": [p.to_dict() for p in hot_posts],
            "bug_count": bug_count,
            "critical_count": critical_count,
            "alert_message": alert_message
        }
